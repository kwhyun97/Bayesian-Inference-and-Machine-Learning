---
title: "Homework 2"
subtitle: "FINM 33210: Bayesian Statistical Inference and Machine Learning"
author: "Ki Hyun"
date: "Due: 23:59 (CT) April 29th 2023"
output: html_notebook
---

```{r packages, message=FALSE}
library(dplyr)
library(purrr)
library(glmnet)
```


## Problem 2.2

### (a)

```{r coordinate_descent_func}
coordinate_descent <- function(lambda, y, X){
  # constants
  n <- nrow(X)
  p <- ncol(X)
  # scaling constants
  ## mean values
  y_center <- mean(y)
  X_center <- colMeans(X)
  ## standard deviations
  y_scale <- sd(y)
  X_scale <- apply(X, 2, sd)
  # scaling y and X
  X <- scale(X)
  y <- scale(y)
  # speedup suggested by 2.20
  yX <- crossprod(y, X)
  oneX <- rep(0, times = p) # since X is scaled
  XX <- crossprod(X)
  # speedup for gamma calculation
  X_square_norm <- diag(XX)
  gamma <- n * lambda / X_square_norm
  # remainder of 2.20
  diag(XX) <- 0
  # initial arbitrary guess for betas
  updated_betas <- betas <- rep(0, times = (p + 1))
  # from 2.19
  updated_betas[1] <- 0.0 # with scaled data, beta 0 is constantly 0
  # tolerance to define "not meaningful change"
  tol <- 1e-10
  while(TRUE){
    for(i in 1:p){
      # from 2.17
      zx <- yX[i] - updated_betas[1] * oneX[i] -t(updated_betas[-1]) %*% XX[,i]
      beta_hat <- zx / X_square_norm[i]
      # from 2.18
      updated_betas[i+1] <- sign(beta_hat) * max(c(0, abs(beta_hat) - gamma[i]))
    }
    # checking convergence based on threshold
    if(max(abs(updated_betas - betas)) < tol){
      break
    }
    # save the updated values to betas prior to construction
    betas <- updated_betas
  }
  # back to unscaled coefficients:
  betas[-1] <- updated_betas[-1] * y_scale / X_scale
  # from 2.19
  betas[1] <- y_center - X_center %*% betas[-1]
  return(betas)
}
```

```{r LASSO_solution_path_func}
LASSO <- function(lambdas, y, X){
  map_dfr(lambdas, ~set_names(c(., coordinate_descent(., y, X)), 
                              c("Lambda" ,"(Intercept)", 
                                paste0("X", seq_along(X)))))
}
```

### (b)

We may generate the data based on the given formula:

$$
y = 3x_1 - 17x_2 + 5x_3 + \epsilon, \
\epsilon \sim N(0, 1)
$$

For the $X_1$, $X_2$, and $X_3$, I will use `Sepal Length`, `Sepal Width`, and
`Petal Length` (respectively) from the `iris` data-set.

```{r 2_2_b_data_generator, message=FALSE}
# generating X data
data("iris")
X <- iris[1:3]
# basic dimensions of data
n <- nrow(X)
p <- ncol(X)
colnames(X) <- map_chr(1:p, ~paste0("X", .))
# setting random seed
set.seed(5581)
# producing error
epsilon <- rnorm(n)
# producing y data
coeffs <- c(3, -17, 5)
y <- as.matrix(X) %*% coeffs + epsilon
```

The generated data is shown below:

```{r 2_2_b_generated_data}
cbind(y, X)
```

Now from the coded function in `2.2 (a)`, we may compute the LASSO solution path
for 100 $\lambda$ values. We may do the same using the `glmnet` package in `R`.

```{r 2_2_b_fitting}
lambdas <- 10^seq(-3, 5, length.out = 100)
glm_fit <- glmnet(as.matrix(X), y, lambda = lambdas, standardize = TRUE, 
                  alpha = 1) #alpha is set to 1 in order to be a LASSO
LASSO_fit <- LASSO(lambdas, y, X)
```

Below are the results from the `glmnet` package:

```{r 2_2_b_glmnet}
as.matrix(coef(glm_fit)) %>% 
  t(.) %>% 
  cbind(., "Lambda" = glm_fit$lambda) %>% 
  as_tibble(.) %>% 
  select(5, 1:4) %>%
  arrange(Lambda)
```

Below are the results from the function in `2.2 (a)`:

```{r 2_2_b_coordinate_descent}
LASSO_fit %>% 
  arrange(Lambda)
```

Though not precisely the same at the $10^{-10}$ threshold level, the two methods
yield a similar result.

## Problem 2.3

### (a)

As a Financial Mathematics student, I felt the urge to set the true $\beta$ 
values according to the Fibonacci sequence. Since the 50th Fibonacci number
is too huge, I will repeat each Fibonacci number (starting from the third) 
10 times

$$
\beta_1 = 1 \\
\vdots \\
\beta_{10} = 1 \\
\beta_{11} = 2 \\
\vdots \\
\beta_{20} = 2 \\
\vdots \\
\beta_{41} = 8 \\
\vdots \\
\beta_{50} = 8
$$

Therefore, the linear model, for $i$th observation, would be in the form:

$$
\begin{aligned}
y_i =& x_{1, i} + \cdots + x_{10, i} \\
+& 2 x_{11, i} + \cdots + 2 x_{20, i} \\
+& 3 x_{21, i} + \cdots + 3 x_{30, i} \\
+& 5 x_{31, i} + \cdots + 5 x_{40, i} \\
+& 8 x_{41, i} + \cdots + 8 x_{50, i} \\
+& \epsilon_i
\end{aligned}
$$

Here, 

$$
\epsilon_i \stackrel{\mathrm{iid}}{\sim} \mathcal{N}(0,\sigma^2)
$$

For simplicity reasons, I will set $\sigma = 1$.

### (b)

Continuing on from the values set in `(a)`, I will generate $\mathbf{X}$ values
using data-sets provided by `R`.

Each of $x_i$ will be derived from the Normal distribution with the mean set as
one of the 5 `float` variables in the `mtcars` data-set (i.e., `Miles/Gallon`,
`Displacement(cu.in.)`, `Rear Axle Ratio`, `Weight(1000 lbs)`, 
`Quarter-Mile Time (sec)`). The variance of the normal distribution will be 
determined by the square of the `Petal Width` observations in the `iris` 
data-set.

To be precise: 

$x_1, \cdots, x_{10}$ will be derived from 
$N(mpg_1, pw_1), \cdots N(mpg_{10}, pw_{10})$,

$x_{11}, \cdots, x_{20}$ will be derived from 
$N(disp_1, pw_{11}), \cdots N(disp_{10}, pw_{20})$,

$x_{21}, \cdots, x_{30}$ will be derived from 
$N(drat_1, pw_{21}), \cdots N(drat_{10}, pw_{30})$,

$x_{31}, \cdots, x_{40}$ will be derived from 
$N(wt_1, pw_{31}), \cdots N(wt_{10}, pw_{40})$,

$x_{41}, \cdots, x_{50}$ will be derived from 
$N(qsec_1, pw_{41}), \cdots N(qsec_{10}, pw_{50})$

```{r 2_3_b_hyper_params}
data("mtcars")
mus <- c(mtcars$mpg[1:10], mtcars$disp[1:10], mtcars$drat[1:10], 
         mtcars$wt[1:10], mtcars$qsec[1:10])
sigmas <- iris$Petal.Width[1:50]
```

```{r 2_3_b_generate_data, message=FALSE}
set.seed(1212)
sets <- 100
n <- 50
betas <- c(rep(1, 10), rep(2, 10), rep(3, 10), rep(5, 10), rep(8, 10))
X <- map2_dfc(mus, sigmas, ~rnorm(sets*n, mean = .x, sd = .y))
colnames(X) <- paste0("X", seq_along(X))
epsilon <- rnorm(sets * n)
y <- as.matrix(X) %*% betas + epsilon
```

Independence condition is already met from generating the data.
I will simply split the generated 5000 observations into 100 training sets, 
each of size 50.

```{r 2_3_b_training_sets}
data <- cbind(y, X)
# Split the observations into 100 training sets of size 50 each
train_sets <- split(data, rep(1:100, each = 50))
```



